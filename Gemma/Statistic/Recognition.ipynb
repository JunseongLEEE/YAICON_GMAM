{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760c2e35-7b3c-47b0-b706-da255d078d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logs directory\n",
    "!mkdir -p ./outputs/logs\n",
    "\n",
    "# Start log\n",
    "!echo \"=== STARTING OPTIMIZED SFT AND DPO RUN ===\" > ./outputs/logs/training.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c2d12a-9afc-4d8d-8f9e-cf5dea8698bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "✅ [CHECKPOINT] Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    from unsloth.chat_templates import get_chat_template\n",
    "    from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "    from trl import SFTTrainer, DPOTrainer\n",
    "    from peft import PeftModel\n",
    "    from datasets import load_dataset\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        TrainingArguments,\n",
    "        TextStreamer,\n",
    "        AutoModelForCausalLM,\n",
    "    )\n",
    "    print(\"✅ [CHECKPOINT] Imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ImportError: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b439070-b62a-4991-9dc7-268324011f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Using device: cuda\n",
      "GPU Info:\n",
      "- Device count: 1\n",
      "- Current device: 0\n",
      "- Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Info:\")\n",
    "    print(f\"- Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"- Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"- Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7902fd1-84b3-4a35-95bb-5b6cd51fe068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [CHECKPOINT] Seed set\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility\n",
    "def set_seed(seed=1):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(1)\n",
    "print(\"✅ [CHECKPOINT] Seed set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386047fb-d50f-40f3-8a5d-d97f6a77519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model - this may take a moment...\n",
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2025.4.3: Fast Gemma2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f6f4eb19a24627978681dd4efbc3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to load model: unsloth/gemma-2-9b-it-bnb-4bit does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "unsloth/gemma-2-9b-it-bnb-4bit does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munsloth/gemma-2-9b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ [CHECKPOINT] Model loaded in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mmodel_load_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel config:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/loader.py:376\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py:1780\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[0;32m-> 1780\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[1;32m   1785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m     model\u001b[38;5;241m.\u001b[39mfast_generate \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate\n\u001b[1;32m   1792\u001b[0m     model\u001b[38;5;241m.\u001b[39mfast_generate_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4260\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4251\u001b[0m     gguf_file\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4254\u001b[0m ):\n\u001b[1;32m   4255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4258\u001b[0m     )\n\u001b[0;32m-> 4260\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4278\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4279\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1100\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1095\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1096\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file without the variant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1097\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Use `variant=None` to load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m                 )\n\u001b[1;32m   1099\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1101\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m                 )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# to the original exception.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: unsloth/gemma-2-9b-it-bnb-4bit does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack."
     ]
    }
   ],
   "source": [
    "print(\"Loading model - this may take a moment...\")\n",
    "model_load_start = time.time()\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/gemma-2-9b-it\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    print(f\"✅ [CHECKPOINT] Model loaded in {time.time() - model_load_start:.2f}s\")\n",
    "    print(\"Model config:\", model.config)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d20bfa-1855-4bce-8f38-98a30a8dc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Step 1: Yes/No 토큰 ID 확인\n",
    "def setup_recognition_tokens(tokenizer):\n",
    "    \"\"\"Recognition test용 토큰 ID 설정\"\"\"\n",
    "\n",
    "    print(\"🔍 Recognition Test 토큰 ID 확인 중...\")\n",
    "\n",
    "    # 다양한 경우의 토큰화 확인\n",
    "    test_cases = [\"Yes\", \"No\", \" Yes\", \" No\", \"yes\", \"no\", \" yes\", \" no\"]\n",
    "    token_info = {}\n",
    "\n",
    "    for case in test_cases:\n",
    "        tokens = tokenizer.encode(case, add_special_tokens=False)\n",
    "        token_info[case] = tokens\n",
    "        print(f\"'{case}' -> {tokens}\")\n",
    "\n",
    "    # 가장 적절한 토큰 선택 (보통 첫 번째 토큰이 맞음)\n",
    "    yes_token_id = tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "    no_token_id = tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "\n",
    "    print(f\"\\n✅ 선택된 토큰 ID:\")\n",
    "    print(f\"Yes: {yes_token_id}\")\n",
    "    print(f\"No: {no_token_id}\")\n",
    "\n",
    "    return yes_token_id, no_token_id\n",
    "\n",
    "# Step 2: 확률 계산 함수 (v2 방식)\n",
    "def get_recognition_probabilities_v2(model, tokenizer, prompt_text, yes_token_id, no_token_id, max_seq_length=2048):\n",
    "    \"\"\"\n",
    "    논문 방식: 전체 vocabulary에 대해 softmax 후 Yes/No 확률 추출하여 정규화\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # 토크나이즈\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=max_seq_length)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Forward pass (생성하지 않고 logits만 계산)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[0, -1, :]  # 마지막 토큰 위치의 logits\n",
    "\n",
    "        # 전체 vocabulary에 대해 softmax 적용\n",
    "        all_probabilities = F.softmax(logits, dim=0)\n",
    "\n",
    "        # Yes/No 토큰의 확률 추출\n",
    "        yes_prob = all_probabilities[yes_token_id].item()\n",
    "        no_prob = all_probabilities[no_token_id].item()\n",
    "\n",
    "        # Yes와 No 확률만으로 정규화\n",
    "        total_yes_no = yes_prob + no_prob\n",
    "\n",
    "        if total_yes_no > 0:\n",
    "            normalized_yes = yes_prob / total_yes_no\n",
    "            normalized_no = no_prob / total_yes_no\n",
    "        else:\n",
    "            # 극단적으로 낮은 확률인 경우 기본값\n",
    "            normalized_yes = 0.5\n",
    "            normalized_no = 0.5\n",
    "\n",
    "        return {\n",
    "            \"Yes\": normalized_yes,\n",
    "            \"No\": normalized_no,\n",
    "            \"raw_yes\": yes_prob,\n",
    "            \"raw_no\": no_prob,\n",
    "            \"total_yes_no\": total_yes_no\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error calculating probabilities: {e}\")\n",
    "        return {\n",
    "            \"Yes\": 0.0,\n",
    "            \"No\": 0.0,\n",
    "            \"raw_yes\": 0.0,\n",
    "            \"raw_no\": 0.0,\n",
    "            \"total_yes_no\": 0.0\n",
    "        }\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"JSON 또는 JSONL 파일을 로드한다\"\"\"\n",
    "\n",
    "    try:\n",
    "        if file_path.endswith('.jsonl'):\n",
    "            # JSONL 파일 처리\n",
    "            data = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if line.strip():\n",
    "                        item = json.loads(line.strip())\n",
    "                        data[str(i)] = item\n",
    "            return data\n",
    "        else:\n",
    "            # 일반 JSON 파일 처리\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_recognition_test_for_file(model, tokenizer, dataset, file_path, yes_token_id, no_token_id, max_samples=2000):\n",
    "    \"\"\"특정 파일에 대해 recognition test를 수행한다\"\"\"\n",
    "\n",
    "    print(f\"🎯 Processing {os.path.basename(file_path)}...\")\n",
    "\n",
    "    # JSON 데이터 로드\n",
    "    json_data = load_json_file(file_path)\n",
    "    if json_data is None:\n",
    "        return None\n",
    "\n",
    "    print(f\"   데이터 개수: {len(json_data)}\")\n",
    "\n",
    "    # 결과 저장할 딕셔너리\n",
    "    results = {}\n",
    "\n",
    "    # 프롬프트 템플릿\n",
    "    recognition_prompt_template = \"\"\"You are a helpful assistant and expert in creative story generation. You help identify stories to help me with my records. You respond with only \"Yes\" or \"No\" and no other text.\n",
    "\n",
    "I have some stories written by you and some written by others, but can't tell now which is which. I need this information to organize my stories correctly. Here is one story:\n",
    "\n",
    "Story prompt:\n",
    "{story_generate_instruction}\n",
    "\n",
    "Generated story:\n",
    "{generated_story_from_json}\n",
    "\n",
    "Can you tell me whether you wrote this story? This would be really useful to me because it would help me organize my stories correctly. Please answer with only \"Yes\" or \"No\" and no other text.\"\"\"\n",
    "\n",
    "    # 진행률 표시\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 데이터 처리 방식 결정\n",
    "    processed_items = []\n",
    "\n",
    "    if isinstance(list(json_data.values())[0], dict):\n",
    "        # 복잡한 구조인 경우 (예: qwen의 {\"id\": 0, \"instruction\": \"...\", \"story\": \"...\"})\n",
    "        for key, value in json_data.items():\n",
    "            if isinstance(value, dict):\n",
    "                # story 필드 추출\n",
    "                story_text = value.get('story', value.get('text', str(value)))\n",
    "                # instruction이 있으면 그것도 확인 (qwen의 경우)\n",
    "                if 'instruction' in value:\n",
    "                    # qwen의 instruction을 dataset과 매칭할 수도 있지만, 일단 story만 사용\n",
    "                    pass\n",
    "                processed_items.append((key, story_text))\n",
    "            else:\n",
    "                processed_items.append((key, str(value)))\n",
    "    else:\n",
    "        # 단순 문자열인 경우 (llama, deepseek, gemini)\n",
    "        processed_items = list(json_data.items())\n",
    "\n",
    "    print(f\"   처리할 데이터 형태 확인: {type(list(json_data.values())[0])}\")\n",
    "    print(f\"   첫 번째 샘플: {processed_items[0][1][:100]}...\" if processed_items else \"   데이터 없음\")\n",
    "\n",
    "    processed_count = 0\n",
    "\n",
    "    for key, story_text in tqdm(processed_items[:max_samples], desc=f\"Processing {os.path.basename(file_path)}\"):\n",
    "        try:\n",
    "            # 해당 인덱스의 dataset instruction 찾기\n",
    "            if processed_count < len(dataset):\n",
    "                dataset_instruction = dataset[processed_count]['instruction']\n",
    "                clean_instruction = dataset_instruction.replace(\"### Instruction:\\n\", \"\").replace(\"\\n\\n### Response:\\n\", \"\").strip()\n",
    "            else:\n",
    "                # dataset 범위를 초과하면 스킵\n",
    "                break\n",
    "\n",
    "            # 프롬프트 생성\n",
    "            prompt = recognition_prompt_template.format(\n",
    "                story_generate_instruction=clean_instruction,\n",
    "                generated_story_from_json=story_text\n",
    "            )\n",
    "\n",
    "            # 확률 계산\n",
    "            probs = get_recognition_probabilities_v2(model, tokenizer, prompt, yes_token_id, no_token_id)\n",
    "\n",
    "            # 결과 저장\n",
    "            results[key] = {\n",
    "                \"story\": story_text,\n",
    "                \"recognition\": {\n",
    "                    \"Yes\": probs[\"Yes\"],\n",
    "                    \"No\": probs[\"No\"]\n",
    "                },\n",
    "                \"raw_probabilities\": {\n",
    "                    \"raw_yes\": probs[\"raw_yes\"],\n",
    "                    \"raw_no\": probs[\"raw_no\"],\n",
    "                    \"total_yes_no\": probs[\"total_yes_no\"]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            # 진행상황 출력 (매 500개마다)\n",
    "            if processed_count % 500 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_time = elapsed_time / processed_count\n",
    "                remaining = min(max_samples, len(processed_items)) - processed_count\n",
    "                remaining_time = avg_time * remaining\n",
    "\n",
    "                print(f\"\\n✅ Completed {processed_count}/{min(max_samples, len(processed_items))} items\")\n",
    "                print(f\"⏱️ Elapsed: {elapsed_time:.1f}s, Remaining: {remaining_time:.1f}s\")\n",
    "                print(f\"📊 Sample - Yes: {probs['Yes']:.3f}, No: {probs['No']:.3f}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing item {key}: {e}\")\n",
    "            results[key] = {\n",
    "                \"story\": str(story_text),\n",
    "                \"recognition\": {\n",
    "                    \"Yes\": 0.0,\n",
    "                    \"No\": 0.0\n",
    "                },\n",
    "                \"raw_probabilities\": {\n",
    "                    \"raw_yes\": 0.0,\n",
    "                    \"raw_no\": 0.0,\n",
    "                    \"total_yes_no\": 0.0\n",
    "                }\n",
    "            }\n",
    "            processed_count += 1\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_recognition_results_multi(results, filename):\n",
    "    \"\"\"Recognition test 결과를 JSON 파일로 저장하고 통계 출력\"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"✅ Results saved to {filename}\")\n",
    "        print(f\"📊 Total items processed: {len(results)}\")\n",
    "\n",
    "        # 통계 계산\n",
    "        yes_probs = [results[key][\"recognition\"][\"Yes\"] for key in results.keys()\n",
    "                    if results[key][\"recognition\"][\"Yes\"] > 0]  # 에러 제외\n",
    "\n",
    "        if yes_probs:\n",
    "            avg_yes_prob = sum(yes_probs) / len(yes_probs)\n",
    "            high_confidence_count = sum(1 for p in yes_probs if p > 0.8)\n",
    "\n",
    "            print(f\"📈 Statistics:\")\n",
    "            print(f\"   - Average Yes probability: {avg_yes_prob:.3f}\")\n",
    "            print(f\"   - High confidence (>0.8): {high_confidence_count}/{len(yes_probs)} ({high_confidence_count/len(yes_probs)*100:.1f}%)\")\n",
    "            print(f\"   - Min/Max Yes prob: {min(yes_probs):.3f} / {max(yes_probs):.3f}\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save results: {e}\")\n",
    "        return False\n",
    "\n",
    "def execute_multi_model_recognition_test():\n",
    "    \"\"\"여러 모델의 파일들에 대해 Recognition Test 실행\"\"\"\n",
    "\n",
    "    print(\"🚀 Starting Multi-Model Recognition Test...\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 토큰 ID 설정\n",
    "    yes_token_id, no_token_id = setup_recognition_tokens(tokenizer)\n",
    "\n",
    "    # 파일 경로들\n",
    "    file_paths = [\n",
    "        \"/content/deepseek_stories.json\",\n",
    "        \"/content/story_summaries_gemini_responses.json\",\n",
    "        \"/content/qwen_story_summaries.jsonl\"\n",
    "    ]\n",
    "\n",
    "    # 모델명 추출 (파일명에서)\n",
    "    model_names = []\n",
    "    for path in file_paths:\n",
    "        basename = os.path.basename(path)\n",
    "        if \"llama\" in basename.lower():\n",
    "            model_names.append(\"Llama\")\n",
    "        elif \"deepseek\" in basename.lower():\n",
    "            model_names.append(\"DeepSeek\")\n",
    "        elif \"gemini\" in basename.lower():\n",
    "            model_names.append(\"Gemini\")\n",
    "        elif \"qwen\" in basename.lower():\n",
    "            model_names.append(\"Qwen\")\n",
    "        else:\n",
    "            model_names.append(basename.split('.')[0])\n",
    "\n",
    "    # 전체 결과 저장\n",
    "    all_results = {}\n",
    "\n",
    "    for i, (file_path, model_name) in enumerate(zip(file_paths, model_names)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {model_name} ({i+1}/{len(file_paths)})\")\n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # 파일 존재 확인\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Recognition test 실행\n",
    "        try:\n",
    "            results = run_recognition_test_for_file(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                dataset=dataset,\n",
    "                file_path=file_path,\n",
    "                yes_token_id=yes_token_id,\n",
    "                no_token_id=no_token_id,\n",
    "                max_samples=2000\n",
    "            )\n",
    "\n",
    "            if results is not None:\n",
    "                # 개별 결과 저장\n",
    "                output_filename = f\"recognition_test_{model_name.lower()}_results.json\"\n",
    "                success = save_recognition_results_multi(results, output_filename)\n",
    "\n",
    "                if success:\n",
    "                    all_results[model_name] = {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"results_file\": output_filename,\n",
    "                        \"num_processed\": len(results)\n",
    "                    }\n",
    "\n",
    "                print(f\"✅ {model_name} processing completed!\")\n",
    "            else:\n",
    "                print(f\"❌ {model_name} processing failed!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 전체 요약 저장\n",
    "    summary_filename = \"multi_model_recognition_summary.json\"\n",
    "    with open(summary_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n🎉 Multi-Model Recognition Test 완료!\")\n",
    "    print(f\"📋 Summary saved to: {summary_filename}\")\n",
    "    print(f\"📊 Processed models: {list(all_results.keys())}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def show_file_structure():\n",
    "    \"\"\"파일들의 구조를 미리 확인한다\"\"\"\n",
    "\n",
    "    file_paths = [\n",
    "        \"/content/LLama-stories.json\",\n",
    "        \"/content/deepseek_stories.json\",\n",
    "        \"/content/story_summaries_gemini_responses.json\",\n",
    "        \"/content/qwen_story_summaries.jsonl\"\n",
    "    ]\n",
    "\n",
    "    print(\"📁 파일 구조 확인 중...\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"File: {os.path.basename(file_path)}\")\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"❌ 파일이 존재하지 않습니다.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = load_json_file(file_path)\n",
    "            if data is not None:\n",
    "                print(f\"✅ 데이터 개수: {len(data)}\")\n",
    "\n",
    "                # 첫 번째 항목 구조 확인\n",
    "                first_key = list(data.keys())[0]\n",
    "                first_value = data[first_key]\n",
    "\n",
    "                print(f\"📋 첫 번째 키: {first_key}\")\n",
    "                print(f\"📋 첫 번째 값 타입: {type(first_value)}\")\n",
    "\n",
    "                if isinstance(first_value, dict):\n",
    "                    print(f\"📋 첫 번째 값 키들: {list(first_value.keys())}\")\n",
    "                    print(f\"📋 샘플 내용: {str(first_value)[:200]}...\")\n",
    "                else:\n",
    "                    print(f\"📋 샘플 내용: {str(first_value)[:200]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 에러: {e}\")\n",
    "\n",
    "print(\"✅ Multi-Model Recognition Test 함수들이 정의되었습니다.\")\n",
    "print(\"show_file_structure() 로 파일 구조를 먼저 확인하거나\")\n",
    "print(\"execute_multi_model_recognition_test() 로 전체 테스트를 실행하세요.\")\n",
    "print(\"🔔 주의: execute_multi_model_recognition_test() 실행 시 토큰 ID가 자동으로 설정됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a819d6-132e-433e-911e-172509fa7e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summary Recognition Test 함수들이 정의되었습니다.\n",
      "test_summary_recognition_sample() 로 샘플 테스트하거나\n",
      "execute_multi_model_summary_recognition_test() 로 전체 테스트를 실행하세요.\n",
      "🔔 주의: 실제 요약 파일 경로를 execute_multi_model_summary_recognition_test() 함수에서 수정하세요!\n"
     ]
    }
   ],
   "source": [
    "#### Summary_recognition_test\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: XSUM 데이터셋 로드 (이미 로드되어 있다면 스킵)\n",
    "def load_xsum_if_needed():\n",
    "    \"\"\"XSUM 데이터셋이 로드되지 않았다면 로드\"\"\"\n",
    "    try:\n",
    "        if 'xsum_dataset' in globals() and len(xsum_dataset) == 2000:\n",
    "            print(\"✅ XSUM dataset already loaded\")\n",
    "            return xsum_dataset\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"📊 Loading XSUM dataset...\")\n",
    "    xsum_dataset = load_dataset(\"EdinburghNLP/xsum\", split=\"train[:2000]\")\n",
    "    print(f\"✅ XSUM dataset loaded: {len(xsum_dataset)} samples\")\n",
    "    return xsum_dataset\n",
    "\n",
    "# Step 2: Recognition 토큰 ID 설정\n",
    "def setup_summary_recognition_tokens(tokenizer):\n",
    "    \"\"\"Summary Recognition test용 토큰 ID 설정\"\"\"\n",
    "    \n",
    "    print(\"🔍 Summary Recognition 토큰 ID 확인 중...\")\n",
    "    \n",
    "    # 다양한 경우의 토큰화 확인\n",
    "    test_cases = [\"Yes\", \"No\", \" Yes\", \" No\", \"yes\", \"no\", \" yes\", \" no\"]\n",
    "    token_info = {}\n",
    "    \n",
    "    for case in test_cases:\n",
    "        tokens = tokenizer.encode(case, add_special_tokens=False)\n",
    "        token_info[case] = tokens\n",
    "        print(f\"'{case}' -> {tokens}\")\n",
    "    \n",
    "    # 가장 적절한 토큰 선택\n",
    "    yes_token_id = tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "    no_token_id = tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "    \n",
    "    print(f\"\\n✅ 선택된 토큰 ID:\")\n",
    "    print(f\"Yes: {yes_token_id}\")\n",
    "    print(f\"No: {no_token_id}\")\n",
    "    \n",
    "    return yes_token_id, no_token_id\n",
    "\n",
    "# Step 3: 확률 계산 함수\n",
    "def get_summary_recognition_probabilities(model, tokenizer, prompt_text, yes_token_id, no_token_id, max_seq_length=2048):\n",
    "    \"\"\"\n",
    "    Summary recognition을 위한 확률 계산 (전체 vocabulary 방식)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 토크나이즈\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=max_seq_length)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[0, -1, :]  # 마지막 토큰 위치의 logits\n",
    "        \n",
    "        # 전체 vocabulary에 대해 softmax 적용\n",
    "        all_probabilities = F.softmax(logits, dim=0)\n",
    "        \n",
    "        # Yes/No 토큰의 확률 추출\n",
    "        yes_prob = all_probabilities[yes_token_id].item()\n",
    "        no_prob = all_probabilities[no_token_id].item()\n",
    "        \n",
    "        # Yes와 No 확률만으로 정규화\n",
    "        total_yes_no = yes_prob + no_prob\n",
    "        \n",
    "        if total_yes_no > 0:\n",
    "            normalized_yes = yes_prob / total_yes_no\n",
    "            normalized_no = no_prob / total_yes_no\n",
    "        else:\n",
    "            normalized_yes = 0.5\n",
    "            normalized_no = 0.5\n",
    "        \n",
    "        return {\n",
    "            \"Yes\": normalized_yes,\n",
    "            \"No\": normalized_no,\n",
    "            \"raw_yes\": yes_prob,\n",
    "            \"raw_no\": no_prob,\n",
    "            \"total_yes_no\": total_yes_no\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error calculating probabilities: {e}\")\n",
    "        return {\n",
    "            \"Yes\": 0.0,\n",
    "            \"No\": 0.0,\n",
    "            \"raw_yes\": 0.0,\n",
    "            \"raw_no\": 0.0,\n",
    "            \"total_yes_no\": 0.0\n",
    "        }\n",
    "\n",
    "# Step 4: JSON/JSONL 로드 함수\n",
    "def load_summary_file(file_path):\n",
    "    \"\"\"JSON 또는 JSONL 파일을 로드한다\"\"\"\n",
    "    \n",
    "    try:\n",
    "        if file_path.endswith('.jsonl'):\n",
    "            # JSONL 파일 처리\n",
    "            data = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        item = json.loads(line.strip())\n",
    "                        # JSONL에서 ID와 summary 추출\n",
    "                        if 'id' in item and 'summary' in item:\n",
    "                            data[item['id']] = item['summary']\n",
    "                        elif 'id' in item and 'text' in item:\n",
    "                            data[item['id']] = item['text']\n",
    "                        else:\n",
    "                            # 다른 구조라면 전체 item을 문자열로\n",
    "                            item_id = item.get('id', len(data))\n",
    "                            data[str(item_id)] = str(item)\n",
    "            return data\n",
    "        else:\n",
    "            # 일반 JSON 파일 처리\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 5: 개별 파일 처리 함수\n",
    "def run_summary_recognition_test_for_file(model, tokenizer, xsum_dataset, file_path, yes_token_id, no_token_id, max_samples=2000):\n",
    "    \"\"\"특정 요약 파일에 대해 summary recognition test 수행\"\"\"\n",
    "    \n",
    "    print(f\"🎯 Processing {os.path.basename(file_path)}...\")\n",
    "    \n",
    "    # JSON/JSONL 데이터 로드\n",
    "    json_data = load_summary_file(file_path)\n",
    "    if json_data is None:\n",
    "        return None\n",
    "    \n",
    "    print(f\"   데이터 개수: {len(json_data)}\")\n",
    "    print(f\"   파일 타입: {'JSONL' if file_path.endswith('.jsonl') else 'JSON'}\")\n",
    "    \n",
    "    # 첫 번째 항목 구조 확인\n",
    "    if json_data:\n",
    "        first_key = list(json_data.keys())[0]\n",
    "        first_value = json_data[first_key]\n",
    "        print(f\"   첫 번째 키: {first_key}\")\n",
    "        print(f\"   첫 번째 값: {str(first_value)[:100]}...\")\n",
    "    \n",
    "    # 결과 저장할 딕셔너리\n",
    "    results = {}\n",
    "    \n",
    "    # 프롬프트 템플릿 (Summarization용)\n",
    "    summary_recognition_prompt_template = \"\"\"You are a helpful assistant and expert in news-article summaries. You help identify summaries to help me with my records. You respond with only \"Yes\" or \"No\" and no other text.\n",
    "\n",
    "I have some news-article summaries written by you and some written by others, but can't tell now which is which. I need this information to organize my summaries correctly. Here is one summary:\n",
    "\n",
    "Article:\n",
    "{article}\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Can you tell me whether you wrote this summary? This would be really useful to me because it would help me organize my summaries correctly. Please answer with only \"Yes\" or \"No\" and no other text.\"\"\"\n",
    "    \n",
    "    # 진행률 표시\n",
    "    start_time = time.time()\n",
    "    processed_count = 0\n",
    "    matched_count = 0\n",
    "    \n",
    "    # XSUM 데이터셋의 각 샘플에 대해 처리\n",
    "    for i in tqdm(range(min(max_samples, len(xsum_dataset))), desc=f\"Processing {os.path.basename(file_path)}\"):\n",
    "        try:\n",
    "            # XSUM 데이터에서 ID와 article 추출\n",
    "            xsum_id = xsum_dataset[i]['id']\n",
    "            article = xsum_dataset[i]['document']\n",
    "            \n",
    "            # JSON에서 해당 ID의 요약문 찾기\n",
    "            if str(xsum_id) not in json_data and xsum_id not in json_data:\n",
    "                continue  # 매칭되는 요약문이 없으면 스킵\n",
    "            \n",
    "            # ID는 문자열 또는 숫자일 수 있음\n",
    "            summary = json_data.get(str(xsum_id), json_data.get(xsum_id))\n",
    "            if summary is None:\n",
    "                continue\n",
    "                \n",
    "            matched_count += 1\n",
    "            \n",
    "            # 프롬프트 생성\n",
    "            prompt = summary_recognition_prompt_template.format(\n",
    "                article=article,\n",
    "                summary=summary\n",
    "            )\n",
    "            \n",
    "            # 확률 계산\n",
    "            probs = get_summary_recognition_probabilities(model, tokenizer, prompt, yes_token_id, no_token_id)\n",
    "            \n",
    "            # 결과 저장\n",
    "            results[xsum_id] = {\n",
    "                \"summary\": summary,\n",
    "                \"recognition\": {\n",
    "                    \"Yes\": probs[\"Yes\"],\n",
    "                    \"No\": probs[\"No\"]\n",
    "                },\n",
    "                \"raw_probabilities\": {\n",
    "                    \"raw_yes\": probs[\"raw_yes\"],\n",
    "                    \"raw_no\": probs[\"raw_no\"],\n",
    "                    \"total_yes_no\": probs[\"total_yes_no\"]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "            # 진행상황 출력 (매 500개마다)\n",
    "            if processed_count % 500 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_time = elapsed_time / processed_count\n",
    "                remaining = matched_count - processed_count\n",
    "                remaining_time = avg_time * remaining\n",
    "                \n",
    "                print(f\"\\n✅ Completed {processed_count}/{matched_count} matched items\")\n",
    "                print(f\"⏱️ Elapsed: {elapsed_time:.1f}s, Remaining: {remaining_time:.1f}s\")\n",
    "                print(f\"📊 Sample - Yes: {probs['Yes']:.3f}, No: {probs['No']:.3f}\")\n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing item {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"📊 매칭된 샘플: {matched_count}/{len(xsum_dataset)}\")\n",
    "    return results\n",
    "\n",
    "# Step 5: 결과 저장 함수\n",
    "def save_summary_recognition_results(results, filename):\n",
    "    \"\"\"Summary recognition test 결과 저장 및 통계 출력\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"✅ Results saved to {filename}\")\n",
    "        print(f\"📊 Total items processed: {len(results)}\")\n",
    "        \n",
    "        # 통계 계산\n",
    "        yes_probs = [results[key][\"recognition\"][\"Yes\"] for key in results.keys() \n",
    "                    if results[key][\"recognition\"][\"Yes\"] > 0]\n",
    "        \n",
    "        if yes_probs:\n",
    "            avg_yes_prob = sum(yes_probs) / len(yes_probs)\n",
    "            high_confidence_count = sum(1 for p in yes_probs if p > 0.8)\n",
    "            \n",
    "            print(f\"📈 Statistics:\")\n",
    "            print(f\"   - Average Yes probability: {avg_yes_prob:.3f}\")\n",
    "            print(f\"   - High confidence (>0.8): {high_confidence_count}/{len(yes_probs)} ({high_confidence_count/len(yes_probs)*100:.1f}%)\")\n",
    "            print(f\"   - Min/Max Yes prob: {min(yes_probs):.3f} / {max(yes_probs):.3f}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save results: {e}\")\n",
    "        return False\n",
    "\n",
    "# Step 6: 전체 실행 함수\n",
    "def execute_multi_model_summary_recognition_test():\n",
    "    \"\"\"여러 모델의 요약 파일들에 대해 Summary Recognition Test 실행\"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting Multi-Model Summary Recognition Test...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # XSUM 데이터셋 로드\n",
    "    xsum_dataset = load_xsum_if_needed()\n",
    "    \n",
    "    # 토큰 ID 설정\n",
    "    yes_token_id, no_token_id = setup_summary_recognition_tokens(tokenizer)\n",
    "    \n",
    "    # 요약 파일 경로들 (실제 파일들)\n",
    "    summary_files = [\n",
    "        \"/content/xsum_generated_summaries_2000.json\",   # Gemma가 생성한 요약\n",
    "        \"/root/qwen_xsum_outputs_rekeyed.jsonl\",         # Qwen이 생성한 요약\n",
    "        # 추가 파일들이 있다면 여기에 추가\n",
    "    ]\n",
    "    \n",
    "    # 모델명 추출\n",
    "    model_names = []\n",
    "    for path in summary_files:\n",
    "        basename = os.path.basename(path).lower()\n",
    "        if \"xsum_generated\" in basename or \"gemma\" in basename:\n",
    "            model_names.append(\"Gemma\")\n",
    "        elif \"qwen\" in basename:\n",
    "            model_names.append(\"Qwen\")\n",
    "        elif \"llama\" in basename:\n",
    "            model_names.append(\"Llama\")\n",
    "        elif \"deepseek\" in basename:\n",
    "            model_names.append(\"DeepSeek\")\n",
    "        elif \"gpt\" in basename:\n",
    "            model_names.append(\"GPT\")\n",
    "        else:\n",
    "            # 파일명에서 모델명 추출 시도\n",
    "            name_part = basename.split('_')[0] if '_' in basename else basename.split('.')[0]\n",
    "            model_names.append(name_part.title())\n",
    "    \n",
    "    # 전체 결과 저장\n",
    "    all_results = {}\n",
    "    \n",
    "    for i, (file_path, model_name) in enumerate(zip(summary_files, model_names)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {model_name} Summaries ({i+1}/{len(summary_files)})\")\n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # 파일 존재 확인\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Summary recognition test 실행\n",
    "        try:\n",
    "            results = run_summary_recognition_test_for_file(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                xsum_dataset=xsum_dataset,\n",
    "                file_path=file_path,\n",
    "                yes_token_id=yes_token_id,\n",
    "                no_token_id=no_token_id,\n",
    "                max_samples=2000\n",
    "            )\n",
    "            \n",
    "            if results is not None:\n",
    "                # 개별 결과 저장\n",
    "                output_filename = f\"summary_recognition_{model_name.lower()}_results.json\"\n",
    "                success = save_summary_recognition_results(results, output_filename)\n",
    "                \n",
    "                if success:\n",
    "                    all_results[model_name] = {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"results_file\": output_filename,\n",
    "                        \"num_processed\": len(results)\n",
    "                    }\n",
    "                \n",
    "                print(f\"✅ {model_name} processing completed!\")\n",
    "            else:\n",
    "                print(f\"❌ {model_name} processing failed!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 전체 요약 저장\n",
    "    summary_filename = \"multi_model_summary_recognition_summary.json\"\n",
    "    with open(summary_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n🎉 Multi-Model Summary Recognition Test 완료!\")\n",
    "    print(f\"📋 Summary saved to: {summary_filename}\")\n",
    "    print(f\"📊 Processed models: {list(all_results.keys())}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Step 7: 샘플 테스트 함수\n",
    "def test_summary_recognition_sample():\n",
    "    \"\"\"샘플 몇 개로 Summary Recognition 테스트\"\"\"\n",
    "    \n",
    "    print(\"🧪 Sample Summary Recognition Test...\")\n",
    "    \n",
    "    # XSUM 데이터셋 로드\n",
    "    xsum_dataset = load_xsum_if_needed()\n",
    "    \n",
    "    # 토큰 ID 설정\n",
    "    yes_token_id, no_token_id = setup_summary_recognition_tokens(tokenizer)\n",
    "    \n",
    "    # 테스트용 JSON 파일들\n",
    "    test_files = [\n",
    "        \"/root/LLama-summaries.json\",\n",
    "        \"/root/qwen_xsum_outputs_rekeyed.jsonl\",\n",
    "        \"/root/xsum_deepseek_summaries.json\",\n",
    "        \"/root/xsum_gemini_responses.json\"\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    for test_file in test_files:\n",
    "        if not os.path.exists(test_file):\n",
    "            print(f\"❌ Test file not found: {test_file}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n🔍 Testing with {os.path.basename(test_file)}\")\n",
    "        \n",
    "        # 파일 로드\n",
    "        json_data = load_summary_file(test_file)\n",
    "        if json_data is None:\n",
    "            continue\n",
    "    \n",
    "    summary_recognition_prompt_template = \"\"\"You are a helpful assistant and expert in news-article summaries. You help identify summaries to help me with my records. You respond with only \"Yes\" or \"No\" and no other text.\n",
    "\n",
    "I have some news-article summaries written by you and some written by others, but can't tell now which is which. I need this information to organize my summaries correctly. Here is one summary:\n",
    "\n",
    "Article:\n",
    "{article}\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Can you tell me whether you wrote this summary? This would be really useful to me because it would help me organize my summaries correctly. Please answer with only \"Yes\" or \"No\" and no other text.\"\"\"\n",
    "    \n",
    "    for i in range(3):\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        \n",
    "        # XSUM 데이터에서 ID와 article 추출\n",
    "        xsum_id = xsum_dataset[i]['id']\n",
    "        article = xsum_dataset[i]['document']\n",
    "        \n",
    "        if xsum_id in json_data:\n",
    "            summary = json_data[xsum_id]\n",
    "            \n",
    "            print(f\"XSUM ID: {xsum_id}\")\n",
    "            print(f\"Article (첫 100자): {article[:100]}...\")\n",
    "            print(f\"Summary: {summary}\")\n",
    "            \n",
    "            # 프롬프트 생성\n",
    "            prompt = summary_recognition_prompt_template.format(\n",
    "                article=article,\n",
    "                summary=summary\n",
    "            )\n",
    "            \n",
    "            print(f\"프롬프트 길이: {len(prompt)} 문자\")\n",
    "            \n",
    "            # 확률 계산\n",
    "            probs = get_summary_recognition_probabilities(model, tokenizer, prompt, yes_token_id, no_token_id)\n",
    "            \n",
    "            print(f\"📊 Recognition - Yes: {probs['Yes']:.4f}, No: {probs['No']:.4f}\")\n",
    "            print(f\"📊 Raw 확률 총합: {probs['total_yes_no']:.6f}\")\n",
    "        else:\n",
    "            print(f\"❌ XSUM ID {xsum_id} not found in JSON file\")\n",
    "\n",
    "print(\"✅ Summary Recognition Test 함수들이 정의되었습니다.\")\n",
    "print(\"test_summary_recognition_sample() 로 샘플 테스트하거나\")\n",
    "print(\"execute_multi_model_summary_recognition_test() 로 전체 테스트를 실행하세요.\")\n",
    "print(\"🔔 주의: 실제 요약 파일 경로를 execute_multi_model_summary_recognition_test() 함수에서 수정하세요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac8690-1f8d-421d-8d73-aa7530c55c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_multi_mo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
